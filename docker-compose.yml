networks:
  # This allows OS2datascanners admin and admin_runner containers access 
  # to both the default network in OS2datascanner, and in the OS2mo-project,
  # if available. Else, they will only use os2datascanner_default.
  os2mo_default:
    name:
      os2mo_default

services:

  init_SBSYS_DB:
    image: magentaaps/os2datascanner-sbsys-init:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: sbsys-init
    depends_on:
      SBSYS_DB:
        condition: service_healthy
    profiles:
      - sbsys

  SBSYS_DB:
    image: mcr.microsoft.com/mssql/server:2017-latest
    ports:
      - "7777:1433"
      - "7778:1434"
    environment:
      - ACCEPT_EULA=Y
      - MSSQL_SA_PASSWORD=ub3rStr0nGpassword
      - MSSQL_COLLATION=SQL_Danish_Pref_CP1_CI_AS
    healthcheck:
      test: [ "CMD-SHELL", "/opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P ${Sa_Password:-ub3rStr0nGpassword} -Q 'SELECT 1' || exit 1" ]
      interval: 10s
      retries: 3
      start_period: 10s
      timeout: 3s
    profiles:
      - sbsys

  db:
    image: postgres:12
    ports:
        - "8035:5432"
    shm_size: '2GB'
    command: >-
      postgres
       -c shared_buffers=1GB
       -c work_mem=64MB
       -c maintenance_work_mem=499MB
       -c effective_cache_size=8GB
    env_file:
      - dev-environment/db.env
    environment:
      - PGUSER=postgres
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./docker/postgres-initdb.d/10-test-for-valid-env-variables.sh:/docker-entrypoint-initdb.d/10-test-for-valid-env-variables.sh
      - ./docker/postgres-initdb.d/20-create-admin-db-and-user.sh:/docker-entrypoint-initdb.d/20-create-admin-db-and-user.sh
      - ./docker/postgres-initdb.d/40-create-report-db-and-user.sh:/docker-entrypoint-initdb.d/40-create-report-db-and-user.sh
      - ./dev-environment/postgres-initdb.d/50-add-createdb-permissions.sh:/docker-entrypoint-initdb.d/50-add-createdb-permissions.sh
    healthcheck:
      test: ["CMD-SHELL", "pg_isready || false"]
      interval: 5s
      start_period: 30s
      retries: 3

  queue:
    # Normally, we expect the `rabbitmq` image. The -management images come
    # with a set of management plugins installed and enabled by default. They
    # can be accessed through a web interface. The credentials are os2ds/os2ds.
    image: rabbitmq:3.11.1-management-alpine
    hostname: os2datascanner_msg_broker
    environment:
      - RABBITMQ_CONFIG_FILE=/etc/rabbitmq/rabbitmq.conf
      - RABBITMQ_ADVANCED_CONFIG_FILE=/etc/rabbitmq/advanced.config
    ports:
      - "8030:15672"  # management port
      - "8072:5672"  # For local attachment during IDE debugging
    volumes:
      - ./docker/rabbitmq/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf:ro
      - ./docker/rabbitmq/advanced.config:/etc/rabbitmq/advanced.config:ro
      - ./docker/rabbitmq/definitions.json:/etc/rabbitmq/definitions.json:ro
    healthcheck:
      test: ["CMD-SHELL", "rabbitmqctl await_online_nodes 1 || false"]
      interval: 5s
      start_period: 30s
      retries: 3

  frontend:
    build:
      context: .
      # we could also use docker/report/Dockerfile as our dockerfile
      dockerfile: docker/base/Dockerfile
      target: frontend
    volumes:
      # "create" the folder node_modules in the container as otherwise it will
      # be overwritten/removed if ./src/os2datascanner/projects/static on the
      # host does not contain node_modules. node_modules is required for
      # the webpack dev server to run
      - /code/src/os2datascanner/projects/static/node_modules/
      - ./src/os2datascanner/projects/static:/code/src/os2datascanner/projects/static
      - frontend-bundles:/code/src/os2datascanner/projects/static/dist/

  admin_migrate:
    image: magentaaps/os2datascanner-admin:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: admin
    command: ["python", "manage.py", "migrate"]
    volumes:
      - ./dev-environment/admin/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on:
      db:
        condition: service_healthy

  admin:
    image: magentaaps/os2datascanner-admin:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: admin
      args:
       COMMIT_SHA: "${OS2DS_COMMIT_SHA}"
       COMMIT_TAG: "${OS2DS_COMMIT_TAG}"
       CURRENT_BRANCH: "${OS2DS_CURRENT_BRANCH}"
    command: [
      # In production, we use gunicorn with the worker class
      # "uvicorn.workers.UvicornWorker", but that does not work with --reload,
      # so we use uvicorn in development at the cost of some dev/prod parity.
      "uvicorn",
      "--reload",
      "--host", "0.0.0.0",
      "--port", "5000",
      # reload when translations are compiled with `docker-compose exec admin django-admin compilemessages`
      "--reload-include", "*.mo",
      "os2datascanner.projects.admin.asgi:application",
    ]
    tty: # If you like coloured log messages.
      true
    volumes:
      # First line below is needed in dev, because we copy all our live code in, which replaces
      # the built-in code from Dockerfile (this is for watchdog reasons)
      # - /code/src/os2datascanner/core_organizational_structure/
      - frontend-bundles:/code/src/os2datascanner/projects/static/dist/
      - ./dev-environment/admin/dev-settings.toml:/user-settings.toml
      - ./dev-environment/admin/.secret:/code/.secret
      - ./src/os2datascanner:/code/src/os2datascanner
    ports:
      - "8020:5000"
    depends_on: &admin_dependencies
      admin_migrate:
        condition: service_completed_successfully
      frontend:
        condition: service_started
      queue:
        condition: service_healthy
    networks:
      - default
      - os2mo_default

  admin_checkup_collector:
    image: magentaaps/os2datascanner-admin:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: admin
    command: python manage.py checkup_collector
    tty:
      true
    volumes:
      - ./dev-environment/admin/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on: *admin_dependencies

  admin_status_collector:
    restart: unless-stopped
    image: magentaaps/os2datascanner-admin:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: admin
    command: python manage.py status_collector
    tty:
      true
    volumes:
      - ./dev-environment/admin/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on: *admin_dependencies

  admin_job_runner:
    restart: unless-stopped
    image: magentaaps/os2datascanner-admin:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: admin
    environment:
      - AMQP_BROADCAST_SYNC=false
    command: python manage.py run_background_jobs
    tty:
      true
    volumes:
      - ./dev-environment/admin/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on: *admin_dependencies
    stop_grace_period: 1m30s
    networks:
      - default
      - os2mo_default

  report_migrate:
    image: magentaaps/os2datascanner-report:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: report
    command: ["python", "manage.py", "migrate"]
    tty:
      true
    volumes:
      - ./dev-environment/report/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on:
      db:
        condition: service_healthy

  report:
    image: magentaaps/os2datascanner-report:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: report
    command: [
      # In production, we use gunicorn with the worker class
      # "uvicorn.workers.UvicornWorker", but that does not work with --reload,
      # so we use uvicorn in development at the cost of some dev/prod parity.
      "uvicorn",
      "--reload",
      "--host", "0.0.0.0",
      "--port", "5000",
      # reload when translations are compiled with `docker-compose exec report django-admin compilemessages`
      "--reload-include", "*.mo",
      "os2datascanner.projects.report.asgi:application",
    ]
    tty:
      true
    volumes:
      # First line below is needed in dev, because we copy all our live code in, which replaces
      # the built-in code from Dockerfile (this is for watchdog reasons)
      # - /code/src/os2datascanner/core_organizational_structure/
      - frontend-bundles:/code/src/os2datascanner/projects/static/dist/
      - ./dev-environment/report/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
      - ./src/os2datascanner/projects/report/media:/code/uploads/report
      # used for assertion signage see dev-environment/report/dev-settings.toml
      #- ./ssl/signing.crt:/etc/ssl/certs/signing.crt:ro
      #- ./ssl/signing.key:/etc/ssl/certs/signing.key:ro
    ports:
      - "8040:5000"
    depends_on: &report_dependencies
      report_migrate:
        condition: service_completed_successfully
      frontend:
        condition: service_started
      queue:
        condition: service_healthy

  redis:
    # Channel layer backend
    image: redis:latest

  report_event_collector:
    image: magentaaps/os2datascanner-report:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: report
    command: python manage.py event_collector
    tty:
      true
    volumes:
      - ./dev-environment/report/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
      - ./src/os2datascanner/projects/report/media:/code/uploads/report
    depends_on: *report_dependencies

  report_result_collector:
    image: magentaaps/os2datascanner-report:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: report
    command: python manage.py result_collector
    tty:
      true
    volumes:
      - ./dev-environment/report/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on: *report_dependencies

  report_email_tagger:
    image: magentaaps/os2datascanner-report:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: report
    command: python manage.py email_tagger
    tty:
      true
    volumes:
      - ./dev-environment/report/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on: *report_dependencies

  explorer:
    image: magentaaps/os2datascanner-engine:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: engine
    env_file:
      - ./dev-environment/engine/dev-settings.env
    command: explorer
    init: true
    tty:
      true
    volumes:
      - ./dev-environment/engine/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on: &engine_dependencies
      queue:
        condition: service_healthy

  worker:
    image: magentaaps/os2datascanner-engine:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: engine
    command: worker
    init: true
    env_file:
      - ./dev-environment/engine/dev-settings.env
    environment:
      - SCHEDULE_ON_CPU=0
      - QUEUE_SUFFIX
      - ENABLE_RUSAGE=true
    tty:
      true
    volumes:
      - ./dev-environment/engine/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on: *engine_dependencies

  exporter:
    image: magentaaps/os2datascanner-engine:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: engine
    command: exporter
    env_file:
      - ./dev-environment/engine/dev-settings.env
    init: true
    tty:
      true
    volumes:
      - ./dev-environment/engine/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on: *engine_dependencies

  docs:
    image: nicksantamaria/mkdocs
    volumes:
      - ./:/data
    command: ["serve", "-a", "0.0.0.0:8000"]
    ports:
      - "9000:8000"

  api_server:
    image: magentaaps/os2datascanner-api:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: api
    command: [
        "gunicorn",
        "--config", "/code/docker/gunicorn-settings.py",
        "--workers", "2", # only two workers in local dev - to save some resources
        "--reload", # restart workers when code changes
        "os2datascanner.server.wsgi"
    ]
    ports:
      - "8070:5000"
    environment:
      -  OS2DS_ADMIN_USER_CONFIG_PATH="/dev-environment/api/dev-settings.toml"
    tty:
      true
    volumes:
      - ./dev-environment/api/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    profiles:
      - api

  swagger_ui:
    image: swaggerapi/swagger-ui:v3.41.1
    environment:
      - SWAGGER_JSON_URL=http://localhost:8070/openapi.yaml
    ports:
      - "8075:8080"
    depends_on:
      - api_server
    profiles:
      - api

  prometheus:
    image: prom/prometheus
    volumes:
      - "./dev-environment/prometheus.yml:/etc/prometheus/prometheus.yml:ro"
    ports:
      - "8050:9090"
    links:
      - pushgateway:pushgateway
    profiles:
      - metric

  pushgateway:
    image: prom/pushgateway
    ports:
      - "9091:9091"

  # default user is admin/admin
  grafana:
    image: grafana/grafana
    volumes:
      - "./dev-environment/grafana/datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml:ro"
      - "./dev-environment/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro"
    ports:
      - "8060:3000"
    profiles:
      - metric

  idp:
    image: magentaaps/simplesamlphp:master
    environment:
      - SIMPLESAMLPHP_BASEURLPATH=http://localhost:8080/simplesaml/
      - SIMPLESAMLPHP_SP_ENTITY_ID=http://localhost:8040/saml2_auth/acs/
      - SIMPLESAMLPHP_SP_ASSERTION_CONSUMER_SERVICE=http://localhost:8040/saml2_auth/acs/
# Replace the three env variables above with these below, if you wish to test Keycloak.
#      - SIMPLESAMLPHP_SP_ENTITY_ID=http://localkeycloak.os2datascanner:8090/auth/realms/magenta
#      - SIMPLESAMLPHP_SP_ASSERTION_CONSUMER_SERVICE=http://localkeycloak.os2datascanner:8090/auth/realms/magenta/broker/saml/endpoint
#      - SIMPLESAMLPHP_SP_SINGLE_LOGOUT_SERVICE=http://localkeycloak.os2datascanner:8090/auth/realms/magenta/broker/saml/endpoint
    volumes:
      - ./dev-environment/authsources.php:/var/www/simplesamlphp/config/authsources.php
    ports:
      - "8080:8080"
    profiles:
      - sso

  postgres-keycloak:
    image: postgres
    environment:
      POSTGRES_DB: keycloak
      POSTGRES_USER: keycloak
      POSTGRES_PASSWORD: keycloak
    volumes:
      - type: volume
        source: knr-postgres-keycloak-volume
        target: /var/lib/postgresql/data
    profiles:
      - ldap
      - sso

  localkeycloak.os2datascanner:
    image: quay.io/keycloak/keycloak:12.0.0

    # Import of realm is done with migration commands below
    # because we need to predefine the Master realm.
    # Using the KEYCLOAK_IMPORT env variable or the -Dkeycloak.import flag will not
    # work, as it will create a Master realm, then try but fail to create a
    # new Master realm from our json file.

    # migration.strategy=OVERWRITE_EXISTING will drop & create if changes has occurred
    # and is set to our default setting.
    # IGNORE_EXISTING will not import if a realm of this name already exists
    # which could be useful if you need to test other Keycloak settings.
    command: [ "-Djboss.socket.binding.port-offset=10",
               "-Dkeycloak.migration.action=import",
               "-Dkeycloak.migration.provider=singleFile",
               "-Dkeycloak.migration.file=/realm.json",
               "-Dkeycloak.migration.strategy=IGNORE_EXISTING",
             ]
    ports:
      - 8090:8090
    environment:
      KEYCLOAK_USER: admin
      KEYCLOAK_PASSWORD: admin
      # PROXY_ADDRESS_FORWARDING: "true"
      DB_USER: keycloak
      DB_PASSWORD: keycloak
      DB_ADDR: postgres-keycloak
      DB_DATABASE: keycloak
      DB_SCHEMA: public
      DB_VENDOR: POSTGRES
    volumes:
      - ./dev-environment/realm.json:/realm.json
    depends_on:
      - postgres-keycloak
    profiles:
      - ldap
      - sso

  ldap_server:
    image: osixia/openldap
    environment:
      LDAP_ADMIN_PASSWORD: testMAG
      LDAP_BASE_DN: dc=magenta,dc=test
      LDAP_ORGANISATION: Magenta
      LDAP_DOMAIN: magenta.test
    ports:
      - 389:389
    volumes:
      - ldap_data:/var/lib/ldap
      - ldap_config:/etc/ldap/slapd.d
    profiles:
      - ldap

  ldap_server_admin:
    image: osixia/phpldapadmin
    ports:
    - 8100:80
    environment:
      PHPLDAPADMIN_LDAP_HOSTS: ldap_server
      PHPLDAPADMIN_HTTPS: 'false'
    depends_on:
      - ldap_server
    profiles:
      - ldap

  samba:
    image: magentaaps/samba-test:master
    volumes:
      - ./dev-environment/data:/mnt
    ports:
      - 8139:139
      - 8445:445
    environment:
      SMB_USER: os2
      SMB_PASSWD: swordfish
      SMB_SHARE_NAME: e2test
      SMB_SHARE_PATH: /mnt
      SMB_SHARE_BROWSABLE: "no"
      SMB_SHARE_READONLY: "yes"

  nginx:
    image: nginx
    volumes:
      - ./dev-environment/data:/usr/share/nginx/html
      - ./dev-environment/nginx_default.conf:/etc/nginx/conf.d/default.conf
    ports:
      - 8910:80

  datasynth:
    image: egnmagenta/os2datasynth:v0.2.5
    environment:
      - DATASYNTH_HOST=datasynth
    ports:
      - 5010:5010

  mailhog:
    image: mailhog/mailhog:v1.0.0
    ports:
      - 8025:8025

x-disabled:
  admin_cron:
    image: magentaaps/os2datascanner-admin:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: admin
    command: supercronic /code/docker/crontab --now
    environment:
      - OS2DS_SKIP_DJANGO_MIGRATIONS=1
      - TZ=Europe/Copenhagen
      - LOG_LEVEL=debug
    volumes:
      - ./dev-environment/admin/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on:
      - db
      - admin
      - queue

  report_cron:
    image: magentaaps/os2datascanner-report:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: report
    command: supercronic /code/docker/crontab
    environment:
      - OS2DS_SKIP_DJANGO_MIGRATIONS=1
      - TZ=Europe/Copenhagen
    volumes:
      - ./dev-environment/report/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on:
      - db
      - report
      - queue

  processor:
    image: magentaaps/os2datascanner-engine:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: engine
    command: processor --enable-metrics
    init: true
    volumes:
      - ./dev-environment/engine/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on:
      - queue

  matcher:
    image: magentaaps/os2datascanner-engine:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: engine
    command: matcher --enable-metrics
    init: true
    restart: unless-stopped
    volumes:
      - ./dev-environment/engine/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on:
      - queue

  tagger:
    image: magentaaps/os2datascanner-engine:dev
    build:
      context: .
      dockerfile: docker/base/Dockerfile
      target: engine
    command: tagger --enable-metrics
    init: true
    volumes:
      - ./dev-environment/engine/dev-settings.toml:/user-settings.toml
      - ./src/os2datascanner:/code/src/os2datascanner
    depends_on:
      - queue


volumes:
  frontend-bundles:
  postgres-data:
  postgres-initdb.d:
  knr-postgres-keycloak-volume:
  ldap_data:
  ldap_config:
